环境：
    1. 无头浏览器, headlessbrowser 目录下，按照setup.sh执行就行... 注意需要root权限.
    2. mysql， 按照django里的配置mysql数据库和密码
    3. scrapy
    4. 我是在linux下跑的，windows下没试过

测试：
    第一步：配置好了数据库，需要先弄下django，把表建立起来
        Django == 1.8.4， python manage.py  syncdb
        如果不报错，说明OK了

    第二步：启动无头浏览器，如果不在同一台机器上的话，请注意把spider里的地址（127.0.0.1）修改一下
        按照setup来启动

    第三步：修改下spider下setting里的djpath.
         这里是frontend下 baic 目录的绝对路径

    第四步：开始爬取， scrapy list获取到spider，然后scrapy crawl (spider) 开始爬取。
         目前只有 sxdetail 这个爬虫和数据库关联，其他的仅仅是打印出数据而已.


Debug:
    问题：
    【自己爬取数据测试的时候，发现有一些报错，这里的报错是因为元素在部分页面找不到引起 out of index 错误。
    我这里只是简单的把这个报错的URL打印出来了，其实是需要特殊处理的.】
    处理：
    判断一个字段是否存在，如果不存在，则判定这个URL是有问题的，所以一些字段重新去取他的Xpath，然后一些字段设置默认值nil，最后入库。
    问题：
    【点击链接，网页源码里并没有实际的URL，需要js解析】
    处理：
    1. 获取网页源码，获取这些链接上层xpath. 比如 实际需要div/a/@href， 这里就获取到 div/a 的xpath
    2. 通过判断links的长度，进一步确定需要的xpath.
    3. 通过xpath，就可以模拟点击，从而获取到点击后的网页URL
    4. 浏览器打开获取到的URL，分析源码，获取数据.
    Bug:
    通过这种方式，会频繁的打开同一个网站，如果网站有防爬虫机制，会被网站判定为恶意攻击或者恶意访问而被ban.





